# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a modular subtitle translation pipeline for the **UFO Citizen Hearing** series that processes Whisper-generated SRT files through a YAML-based workflow. The system uses LLMs for translation with structured context (topics, terminology, guidelines) to ensure consistent, high-quality subtitle translations.

**Source Material**: 20 episodes of UFO Citizen Hearing testimony, transcribed using Whisper (medium model).

## Core Data Architecture

The project uses a **three-layer file structure**:
- `whisper-medium/` - Original Whisper-generated SRT files (source)
- `input/<episode>/` - Copied SRT files for processing
- `data/<episode>/` - Working YAML/Markdown files (main.yaml, topics.json, terminology.yaml, guidelines.md)
- `data/<episode>/drafts/` - Topic-based translation work files (topic_01.md, topic_02.md, etc.)
- `output/<episode>/` - Exported results (SRT/Markdown/reports)

### Primary Data Files (per episode)

**`data/<episode>/main.yaml`** - The central data file containing:
- All parsed SRT segments with timecodes
- Translation results and status tracking
- Segment-level metadata (topic_id, speaker_group, music tags, etc.)

**`data/<episode>/topics.json`** - Thematic structure:
- Topics with segment ranges (`segment_start`, `segment_end`)
- Per-topic summaries and keywords
- Global episode summary for context

**`data/<episode>/terminology.yaml`** - Multi-sense term definitions:
- Terms can have multiple senses (e.g., "UFO" vs "flying saucer")
- Each sense includes preferred translation, definition, applicable segments/topics
- Used to ensure consistent terminology across translations

**`data/<episode>/guidelines.md`** - Translation style guide:
- Tone and voice requirements (formal, objective, technical)
- Special formatting rules for testimony transcripts
- Episode-specific translation instructions
- Loaded as context for translation models

**`data/<episode>/drafts/topic_XX.md`** - Topic-based translation work files:
- Generated by `prepare_topic_drafts.py` from main.yaml
- Each file covers one topic with its segment range
- Contains source text with empty JSON translation fields: `{"text": "", "confidence": "", "notes": ""}`
- Translators directly edit these files to fill in translations
- Processed by `backfill_translations.py` to update main.yaml

### Configuration Files

`configs/default.yaml` 提供所有共用路徑與模型設定，`configs/<episode>.yaml` 通常只需要：
```yaml
episode_id: UFO-01
# input:
#   srt: input/UFO-01/custom_file.srt  # 若資料夾中有多個 SRT 才需要覆寫
```
腳本會自動於 `input/<episode>/` 中尋找唯一 `.srt` 檔案，輸出與日誌則落在 `data/<episode>/...`、`logs/<episode>/workflow.log`。

## Key Processing Rules

### Status Tracking
Each segment has `translation.status`:
- `pending` - Not yet translated
- `in_progress` - Currently being processed
- `completed` - Translation finished
- `needs_review` - Flagged for manual review
- `approved` - Reviewed and approved

### Resume Support
Tools should check `translation.status` to skip already-completed segments, enabling interrupted workflow continuation.

## Key Processing Rules

### SRT Parsing and Segment Merging
When converting SRT to main.yaml:
- **Sentence completeness priority**: MUST merge segments until sentence has terminal punctuation (`.!?…`)
- **Stop merging**: Once sentence is complete, stop if next entry starts with uppercase (new sentence)
- **Safety limit**: Maximum 10 SRT entries per segment to prevent pathological cases
- **Speaker detection**: `>>` prefix indicates speaker change, increment `speaker_group`
- **Music/sound tags**: Preserve `[MUSIC]` tags in `source_text`, let AI translate them
- **Timecode preservation**: Keep original SRT format (`HH:MM:SS,mmm`)
- **Source tracking**: Record original SRT indices in `metadata.source_entries`

**Note on Whisper SRT**: Whisper-generated transcripts have **low speaker marker coverage** (typically 0-5% of segments have `>>` markers). This is normal and does not affect the translation workflow. The `speaker_group` field will still be populated for all segments (defaulting to group 1 when no markers are detected).

### Topics Generation Flow
1. Export `main.yaml` to JSON with segment markers (`segment_id`, `speaker_group`, `source_text`)
2. Feed to large-context LLM (e.g., Gemini 3 Pro (preview)) using `prompts/topic_analysis_system.txt`
3. Parse LLM output to extract segment ranges, summaries, keywords
4. Generate `topics.json` structure with validation (no gaps, no overlaps, sequential ranges)

### Transcription Error Correction Flow (Whisper-specific)
**Purpose**: Fix Whisper transcription errors before translation to prevent error propagation.

Since Whisper transcripts have 5-10% WER (Word Error Rate), the topic analysis LLM also identifies potential transcription errors in `topics.json`:

1. **Error Detection** (automatic during topic analysis):
   - LLM analyzes segments and identifies high-confidence errors (proper nouns, technical terms, contextual mismatches)
   - Errors stored in `topics.json` under each topic's `potential_errors` field
   - Each error includes: `segment_id`, `error_text`, `suggested_correction`, `reasoning`

2. **Error Correction** (manual or automated):
   - Review `potential_errors` in `topics.json`
   - Run `fix_transcription_errors.py` to automatically apply corrections to `main.yaml`
   - Regenerate `main_segments.json` to reflect corrected text
   - Regenerate translation drafts (`topic_XX.md`) with corrected source text

3. **Validation**:
   - Check corrected segments in translation drafts
   - Verify terminology consistency
   - Ensure proper nouns are correctly spelled

**This step is CRITICAL for Whisper transcripts** - fixing errors before translation prevents cascading mistakes in the translated output.

### Terminology Mapping (Candidate Generation)
The `terminology_mapper.py` tool generates `terminology_candidates.yaml`:
- Scans `main.yaml` to find all occurrences of terms from `terminology_template.yaml` and `topics.json` keywords
- For each term, records all matching segments with `segment_id`, `sources` (template/topic), and `source_text`
- Outputs `occurrences` arrays (not yet classified into senses)
- This is the **input** for the manual/AI classification step that produces `terminology.yaml`

The subsequent classification step (manual or via Claude Code):
- Reviews each occurrence's `source_text` context
- Assigns each `segment_id` to the appropriate sense based on semantic meaning
- Produces `terminology.yaml` with `segments` arrays (classified and sense-specific)
- Performs multi-sense disambiguation based on context

## Tool Design Principles

### Command-Line Interface
All tools should accept:
- `--config configs/<episode>.yaml` as primary configuration
- Optional CLI overrides for specific parameters
- `--force` to overwrite existing outputs

### Error Handling
- Log unparseable timecodes but continue processing
- Mark problematic segments with `status: error` or `metadata.truncated: true`
- Never silently fail; provide actionable error messages
- Use non-zero exit codes for failures

### Incremental Writing
- Write results back to `main.yaml` after each batch
- Avoid accumulating large amounts in memory
- Support partial completion and recovery

## Project Architecture

### Directory Structure
```
whisper-medium/         # Original Whisper SRT files
src/                    # Shared modules
├── clients/           # LLM API clients
│   ├── base_client.py      # Abstract base class
│   ├── gemini_client.py    # Gemini (google-genai SDK)
│   ├── openai_client.py    # OpenAI API
│   └── anthropic_client.py # Anthropic (planned)
├── models.py          # Data models (@dataclass)
└── exceptions.py      # Custom exceptions

tools/                 # CLI tool scripts
├── srt_to_main_yaml.py       # SRT → main.yaml parser
├── main_yaml_to_json.py      # Export segments to JSON
├── topics_analysis_driver.py # LLM topic analysis
├── prepare_topic_drafts.py   # Generate translation work files
├── backfill_translations.py  # Update main.yaml with translations
├── fix_chinese_punctuation.py # QA: Fix punctuation
├── export_srt.py             # Export translated SRT
└── split_srt.py              # Split long subtitles
```

## Development Commands

### Setup
```bash
# Install dependencies (includes google-genai 0.1.0+)
pip install -r requirements.txt

# Configure API keys
cp .env.example .env
# Edit .env and add your API keys:
# - GEMINI_API_KEY (recommended for topic analysis)
# - OPENAI_API_KEY (alternative)
```

### Run Tools
```bash
# Step 1: Convert SRT to main.yaml
PYTHONPATH=. python3 tools/srt_to_main_yaml.py --config configs/UFO-01.yaml [--force] [--verbose]

# Step 2: Export segments to JSON
PYTHONPATH=. python3 tools/main_yaml_to_json.py --config configs/UFO-01.yaml [--pretty] [--verbose]

# Step 3: Generate topics.json (requires API key)
PYTHONPATH=. python3 tools/topics_analysis_driver.py --config configs/UFO-01.yaml [--dry-run] [--verbose]

# Step 3.5: Fix transcription errors (Whisper-specific, RECOMMENDED)
# Review potential_errors in topics.json, then apply corrections to main.yaml
PYTHONPATH=. python3 tools/fix_transcription_errors.py --config configs/UFO-01.yaml [--dry-run] [--verbose]

# After fixing errors, regenerate main_segments.json with corrected text
PYTHONPATH=. python3 tools/main_yaml_to_json.py --config configs/UFO-01.yaml [--pretty] [--verbose]

# Step 4: Generate translation drafts (Markdown work files)
# This creates data/<episode>/drafts/topic_01.md, topic_02.md, etc.
# Each file contains source text with empty JSON translation fields
PYTHONPATH=. python3 tools/prepare_topic_drafts.py --config configs/UFO-01.yaml [--force] [--verbose]

# Step 5: Translate (manual editing or with Claude Code/LLM assistance)
# Work files: data/<episode>/drafts/topic_XX.md
# Edit each file to fill in JSON fields:
#   {"text": "翻譯內容", "confidence": "high/medium/low", "notes": "備註說明"}
# Context files available for reference:
#   - configs/guidelines_template.md (translation style guide)
#   - data/<episode>/terminology.yaml (terminology reference, if created)
#   - data/<episode>/topics.json (topic summaries and context)

# Step 5.5: QA - Fix Chinese punctuation (recommended after translation)
# Automatically corrects English punctuation to Chinese in translation text fields
# LLM translations often mix English commas (,) instead of Chinese (，)
PYTHONPATH=. python3 tools/fix_chinese_punctuation.py --config configs/UFO-01.yaml [--dry-run] [--verbose]
# Alternative: manually specify files
# PYTHONPATH=. python3 tools/fix_chinese_punctuation.py data/UFO-01/drafts/topic_*.md [--dry-run] [--verbose]

# Step 6: Backfill completed translations to main.yaml
# Reads completed topic_XX.md files and updates main.yaml with translations
PYTHONPATH=. python3 tools/backfill_translations.py --config configs/UFO-01.yaml [--dry-run] [--verbose]

# Step 7: Export translated SRT subtitles
# Converts main.yaml translations back to SRT format
PYTHONPATH=. python3 tools/export_srt.py --config configs/UFO-01.yaml [--no-speaker-hints] [--fail-on-missing] [--verbose]

# Step 8: Split long subtitle segments (optional but recommended)
# Intelligently splits long segments at punctuation marks for better readability
# IMPORTANT: This tool splits each segment ONCE per run. For very long segments (>100 chars),
# you may need to run it 2-3 times to achieve the target length.
# The tool will automatically report remaining long segments and suggest re-running if needed.

PYTHONPATH=. python3 tools/split_srt.py \
  -i output/UFO-01/UFO-01.zh-TW.srt \
  -o output/UFO-01/UFO-01.zh-TW.split.srt \
  --max-chars 35 \
  [--min-chars 10] \
  [--gap-ms 0] \
  [--verbose]

# If the tool reports remaining long segments, run it again on the output:
# PYTHONPATH=. python3 tools/split_srt.py \
#   -i output/UFO-01/UFO-01.zh-TW.split.srt \
#   -o output/UFO-01/UFO-01.zh-TW.split2.srt \
#   --max-chars 35

# Typical convergence:
# - Run 1: 115 long segments (max 140 chars) → 51 long segments (max 71 chars)
# - Run 2: 51 long segments → 9 long segments (max 47 chars)
# - Run 3: 9 long segments → ~5 long segments (max 44 chars, usually acceptable)
```

### Batch Processing
```bash
# Process all episodes in sequence
for i in {01..20}; do
  echo "Processing UFO-$i..."
  PYTHONPATH=. python3 tools/srt_to_main_yaml.py --config configs/UFO-$i.yaml --verbose
  PYTHONPATH=. python3 tools/main_yaml_to_json.py --config configs/UFO-$i.yaml --verbose
  PYTHONPATH=. python3 tools/topics_analysis_driver.py --config configs/UFO-$i.yaml --verbose
  PYTHONPATH=. python3 tools/prepare_topic_drafts.py --config configs/UFO-$i.yaml --verbose
done
```

## Implementation Status

### Currently Implemented
**Tools:**
- `tools/srt_to_main_yaml.py` - SRT parser with intelligent sentence merging ✅
- `tools/main_yaml_to_json.py` - Export minimal segments for LLM analysis ✅
- `tools/topics_analysis_driver.py` - LLM-based topic analysis with error detection ✅
- `tools/fix_transcription_errors.py` - Fix Whisper transcription errors based on topics.json potential_errors ✅
- `tools/terminology_mapper.py` - Generate terminology candidates from template and topics ✅
- `tools/validate_terminology.py` - Validate terminology.yaml structure ✅
- `tools/prepare_topic_drafts.py` - Generate topic-based translation work files (topic_XX.md) ✅
- `tools/backfill_translations.py` - Read completed topic_XX.md files and update main.yaml with translations ✅
- `tools/fix_chinese_punctuation.py` - QA tool to fix English punctuation in Chinese translations ✅
- `tools/export_srt.py` - Convert main.yaml back to SRT format ✅
- `tools/split_srt.py` - Split long SRT subtitles for better readability ✅

**Shared Modules:**
- `src/clients/base_client.py` - Abstract LLM client interface ✅
- `src/clients/gemini_client.py` - Gemini API (google-genai SDK 0.1.0+) ✅
- `src/clients/openai_client.py` - OpenAI API (GPT-5, Responses API) ✅
- `src/models.py` - Data models (APIResponse, TokenUsage) ✅
- `src/exceptions.py` - Custom exceptions ✅

**Configuration:**
- `configs/default.yaml` - Default configuration for all episodes ✅
- `configs/UFO-01.yaml` to `configs/UFO-20.yaml` - Episode-specific configs ✅
- `configs/guidelines_template.md` - Translation style guide template ✅
- `.env.example` - API key template ✅
- `prompts/topic_analysis_system.txt` - Topic analysis prompt ✅

**Documentation:**
- `README.md` - Project overview and workflow guide ✅
- `EPISODES.md` - Episode mapping and quick reference (20 episodes total) ✅
- `CLAUDE.md` - This file ✅

## Translation Quality Checks

QA tools should validate:
- **Punctuation consistency** - Chinese translations must use Chinese punctuation (，not ,)
- Terminology consistency across segments
- Translation confidence scores
- Text length ratios (source vs. translation)
- Timecode integrity
- Status completeness (all segments translated)
- Segments with `metadata.truncated: true` should be flagged as `needs_review`

**Note:** LLM translations commonly mix English punctuation in Chinese text. Always run `fix_chinese_punctuation.py` after translation (Step 5.5) before backfilling to main.yaml.

## Whisper SRT Characteristics

This project uses Whisper-generated SRT files with the following characteristics:

1. **Low speaker marker coverage**: Only 0-5% of segments have `>>` markers
   - Some episodes (e.g., Rendlesham Forest Part 1) have 50+ markers
   - Others have zero markers
   - This is normal and does not affect the workflow

2. **Automatic sentence segmentation**: Whisper segments audio based on silence detection
   - May not align perfectly with semantic boundaries
   - The merge logic handles this by prioritizing sentence completeness

3. **Average episode length**: ~1,000-1,600 segments per episode

4. **Transcription errors (Word Error Rate ~5-10%)**:
   - ⚠️ **DO NOT treat Whisper transcripts as absolute truth**
   - Common error types:
     - **Proper nouns** frequently misspelled (names, locations, military units, organizations)
     - **Technical terms** misheard as similar-sounding common words
     - **Homophone errors** (their/there, to/too, you're/your)
     - **Word boundary errors** (a part → apart, in to → into)
   - **Impact on translation**:
     - Errors are typically isolated words, not entire sentences
     - Context usually makes the intended meaning clear
     - `topics.json` generated by LLM includes `potential_errors` field with suggested corrections
     - Review and correct obvious errors in source text BEFORE translating to avoid propagating mistakes

**Impact on workflow**: The tools handle low speaker marker coverage gracefully. The `speaker_group` field is always populated (defaults to 1 when no markers detected). Translation draft files will have fewer "Speaker Group" headers, but this is cosmetic and does not affect translation quality.

**Transcription quality workflow**: The `topics_analysis_driver.py` tool uses an LLM to analyze segments and identify potential transcription errors. Review the `potential_errors` field in `topics.json` and correct source text in `main.yaml` before translation to ensure high-quality output.

## Important Notes

- **Episode ID format**: `UFO-01` through `UFO-20` (zero-padded)
- **Episode ID is the primary key** - all file operations use this identifier
- **All documentation is in Traditional Chinese** - except this CLAUDE.md file and code comments
- **API Keys Required** - LLM tools need `.env` file with provider API keys (see `.env.example`)
- **Model Configuration** - Each episode config inherits from `configs/default.yaml`
- **Source files are read-only** - Original Whisper SRT files in `whisper-medium/` are never modified
